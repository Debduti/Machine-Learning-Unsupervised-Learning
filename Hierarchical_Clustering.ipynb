{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxmC9a+wuzl/q4NnPWktbD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debduti/Machine-Learning-Unsupervised-Learning/blob/main/Hierarchical_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcE_-ynidAbB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Clustering\n",
        "\n",
        "Can be braodly divided into two kinds-\n",
        "1. Agglomerative\n",
        "2. Divisive\n",
        "\n",
        "**Agglomerative Hierarchical Clustering**: This is the most common type. It starts with each data point as its own cluster and merges the closest pairs of clusters until only one cluster remains.\n",
        "The algorithm can be described as follows:\n",
        "\n",
        "**a.Initialization**: Start with each data point as a singleton cluster.  \n",
        "\n",
        "**b.Compute Distance Matrix**: Compute the distance between each pair of clusters. The distance between clusters can be calculated using various metrics such as Euclidean distance, Manhattan distance, etc.  \n",
        "\n",
        "**c.Merge Closest Clusters**: Merge the two closest clusters based on the distance matrix.  \n",
        "\n",
        "**d.Update Distance Matrix**: Recalculate the distances between the new cluster and all other clusters.  \n",
        "\n",
        "**e.Repeat**: Repeat steps 3 and 4 until only one cluster remains.  \n",
        "\n",
        "This process creates a dendrogram, a tree-like structure that shows the hierarchical relationships between clusters. At each step of the algorithm, the dendrogram illustrates which clusters are being merged together.\n",
        "\n",
        "The key mathematical concept behind agglomerative hierarchical clustering is the notion of a distance metric between clusters. Once we have a way to measure the distance between clusters, we can iteratively merge the closest ones until we reach the desired number of clusters or until there is only one cluster left.\n",
        "\n",
        "The choice of distance metric can significantly impact the clustering results. Common distance metrics include:\n",
        "\n",
        "**Euclidean Distance**: Measures the straight-line distance between two points in Euclidean space. In 2D space, Euclidean distance = Pythagorian distance.  \n",
        "\n",
        "**Manhattan Distance**: Also known as city block distance, it measures the distance between two points by summing the absolute differences of their coordinates.  \n",
        "\n",
        "**Correlation Distance**: Measures the correlation between two sets of observations, where values close to 1 indicate high similarity and values close to -1 indicate high dissimilarity.\n",
        "These distance metrics can be used to compute the distance between individual data points or between clusters of data points.\n"
      ],
      "metadata": {
        "id": "vfU3hIDQlirY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YaQPK612mImY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}